{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 .SFNS-Regular;}
{\colortbl;\red255\green255\blue255;\red14\green14\blue14;}
{\*\expandedcolortbl;;\cssrgb\c6700\c6700\c6700;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\sl324\slmult1\pardirnatural\partightenfactor0

\f0\b\fs28 \cf2 Track Justification:
\f1\b0 \
This work makes a 
\f0\b methodological and theoretical contribution
\f1\b0  to the field of generative recommender systems. We revisit the interleaved-token formulation underlying Transformer-based recommendation models and provide a first-principles analysis of its strengths and limitations. Guided by the causal relationship between items and user actions, we propose a family of novel attention-based architectures\'97AttnLFA, AttnMVP, and AttnDNA\'97that explicitly encode causality and reduce attention noise. Our designs are theoretically motivated, information-theoretically justified, and empirically validated on large-scale real-world recommendation data, demonstrating improvements in predictive accuracy, model efficiency, and representation learning. The focus on 
\f0\b novel architectures, principled modeling, and rigorous evaluation
\f1\b0  aligns with the Research Track\'92s goals of advancing algorithmic and theoretical foundations in data mining and machine learning, making this work well-suited for the KDD Research Track.}